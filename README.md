# pyEISprocessing
Hi there! pyEISprocessing is a WIP open source project designed to fill the gap in large-scale EIS processing and dataset generation packages. 
- Packages such as impedance.py and pyDRTtools are excellent tools for investigating a small number of impedance datasets.This project extends their functionality to provide easier analysis of large datasets.
- As impedance experiments can be time- or cost-prohibitive, it may be of interest to generate non-experimental data from physical models or equivalent circuits to investigate an electrochemical system. To this end, functions are provided for developing large datasets across multivariate spaces.
- Finally, pyEISprocessing integrates scikit-learn and pyTorch to provide access to machine learning tools; pre-built functions allow for easy, modular access to preprocessing and training.
## Documentation
There is no formal documentation yet. Once I get closer towards a v1.0 of this project, I'll start developing a formal documentation. Currently, many implementations are likely to change. Since I'm the only person working on this, I can kinda get away with playing loose on the documentation for now. You may look through the source code for my slightly deranged comments if you so desire :)
## Features
### Datasets and Model Objects
- Datasets and models are handled as distinct class objects, with the goal of separating dataset assembly and machine learning processing.
- Datasets can be assembled, stored away, and loaded later for data analysis or ML training. This ensures dataset preservation and transferability.
- Experimental data can be loaded from common impedance file formats, or directly from .npy arrays. 
- Datasets can be enhanced with parameters outside of EIS/DRT data. A handful of functions are implemented to derive select parameters I've found important; more will be added in future releases.
- Model objects hold a copy of the dataset that may be transformed during preprocessing. Machine learning models are also stored as an attribute. Multiple models can be trained on a single dataset via model swapping functions.
- TO DO: Generation of impedance data from arbitrary equivalent circuit inputs.
### Equivalent Circuits
- After experimental data is loaded from impedance files, equivalent circuits can be fit to the dataset to investigate a hypothesized model. In addition, Kramers-Kronig relations can be used to validate experimental data. Both functionalities are built on impedance.py, with extensions to scalability and graphical interpretation provided here.
- Once baseline data is established, extended datasets can be generated by varying equivalent circuit parameters.
- Variation functions are scalable and support a range of distribution options.
- [TO DO]: Testing the fit of multiple equivalent circuits simultaneously; trimming base data by linKK or MSE qualifiers.
- [TO DO]: Added noise functions.
- [TO DO]: Data analysis functions, e.g., characteristic frequencies and peak analytics.
### Distribution of Relaxation Times
- Functions are provided for transforming large-scale impedance data into its distribution of relaxation times (DRT) representation. Most tunability present in pyDRTtools can be accessed here, and is applied uniformly over datasets.
- Functions are provided for DRT peak analysis, which can provide lumped values that describe distributions accurately with minimal parameters.
- [TO DO]: Extension of analysis functions.
### Physical Modeling
- The PhysicalModel() class is designed to interface with physical models via a user-created subclass.
- Currently, users simply have to create a custom transformation function that maps variables and constants to an impedance output. Since this is user-defined, it supports arbitrary variable inputs and complexity.
- Similar to equivalent circuits, large datasets can be generated via variation functions. Alternatively, a small number of impedance spectra can be generated by passing explicit variables.
- [TO DO]: Fitting variables to experimental data. This one will probably require a rework of the user function if I utilize scipy.curve_fit() due to the handling of *args in the aforementioned, but I'm looking at alternatives.
- [TO DO]: Fitting multiple physical models to experimental data for validation.
### Machine Learning Training and Evaluation
- The ModelDS() class is designed to store data, transform it for processing (if desired), and store corresponding ML models.
- Primary focus during early development has been on pyTorch integration.
- The preprocessing/training functions [pyt_preprocess_and_train() and preprocess_and_train()] are designed to be highly modular, while also serving as a wrapper for standard preprocessing and training methods.
- [TO DO] Support all scikit-learn models and other training pipelines (e.g., cross validation or K-folds).
- [TO DO] Support classification.
- [TO DO] Extend scoring functions beyond MAPE/r2 (also, support classification scoring)
- [TO DO] Add hyperparameter optimization (similar to scikit-learn's GridSearchCV)
- [WARNING] The pyTorch integration supports complex representation as a beta function. This is due to the current status of complex number inputs in pyTorch; see: https://pytorch.org/docs/stable/complex_numbers.html. It is recommended to use floating point dtypes over complex equivalents.
## Known Issues
- Memory usage with simultaneous usage of Dataset and Model objects remains an issue. This is particularly present during pyTorch preprocessing and training; as many as 4 copies of the dataset may be present in memory at once. I'm looking into solutions.
- DRT processing remains computationally expensive. Currently, it's using (mostly) unmodified pyDRTtools functions to transform each spectrum. It may be possible that parts of this pipeline are extraneous to our needs here.
- Machine learning only supports a few scikit-learn models. In addition, it doesn't support classification yet. This is because my personal work has focused on regression, but classification will be supported soon.
- Other things that I don't want to list because I'm already working on them :)
